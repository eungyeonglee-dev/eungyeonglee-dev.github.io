---
title: "회귀"
excerpt: ""

categories:
    - machine-learning
tags:
    - machine-learning
    - homl
use_math: true
---

# 회귀

회귀 분석은 유전적 특성을 연구하던 영국의 통계학자 갈톤이 수행한 연구에서 유래했다. 부모와 자식 간의 키의 상관관계를 분석했던 갈톤은 부모의 키가 모두 클 때 자식의 키가 크긴 하지만 그렇다고 부모를 능가할 정도로 크지 않았고, 부모의 키가 모두 아주 작을 때 그 자식의 키가 작기는 하지만 부모보다는 큰 경향을 발견했다.

즉, 사람의 키는 평균 키로 회귀하려는 경향을 가진다는 자연의 법칙이 있다. 회귀 분석은 **데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향을 이용한 통계학 기법**이다.

|독립변수 개수|회귀 계수의 결합|
|---|---|
|1개 : 단일 회귀|선형 : 선형 회귀|
|여러 개 : 다중 회귀|비선형 : 비선형 회귀|

## 다항 회귀

비선형 데이터세트에 훈련시킬 수 있는 조금 더 복잡한 모델이다. 다항이다보니 선형 회귀보다 파라미터가 많다. 그래서 훈련 데이터에 과대적합되기 더 쉽다.
- 학습 곡선을 사용해 모델이 과대적합되는지 감지하는 방법
- 훈련 세트의 과대적합을 감소시킬 수 있는 규제 기법

## 분류로 사용되는 회귀

- 로지스틱 회귀
- 소프트 맥스 회귀

## 선형 회귀

다음은 삶의 만족도에 대한 선형 회귀이다.

$\hat{y} = \theta_0+\theta_1 \times 1인당 GDP$
- 1인당 GDP : 모델의 입력 특성
- $\theta_0, \theta_1$ : 모델 파라미터     


***선형 회귀 모델의 예측***    
$\hat{y} = \theta_0 + \theta_1 x_1 + ... + \theta_n x_n$
- $\hat{y}$ : 예측값(종속변수, 결정 값)
- $n$ : 특성의 수
- $x_i$ : $j$번째 특성값(독립변수, 피처)
- $\theta_j$ : $j$번째 모델 파라미터(회귀 계수)

**회귀 예측의 핵심은 주어진 특성값과 예측 값 데이터 기반에서 학습을 통해 <u>최적의 모델 파라미터</u>를 찾아내는 것이다.**

***선형 회귀 모델의 예측(벡터 형태)***      
$\hat{y}=h_\theta \left( \mathbf{x} \right) = \boldsymbol{\theta} \cdot \mathbf{x}$
- $\mathbf{\theta}$ : 모델의 파라미터 벡터(편향 $\theta_0$ 과 $\theta_1$에서 $\theta_n$까지의 특성 가중치)
- $\mathbf{x}$ : 특성 벡터($x_0$에서 $x_n$까지. $x_0$의 값은 1이다.)

***선형 회귀 모델을 만든다.***    
-> ***훈련 데이터를 학습시켜 선형 회귀 모델을 만든다.***
= ***모델이 훈련 세트에 가장 잘 맞도록 모델 파라미터를 설정한다.***
= ***모델이 훈련 데이터에 얼마나 잘 들어맞는지 측정한다.***

우리가 1-3장에서 살펴본 것은 프로젝트의 전체 단계이다. 이는 데이터를 sklearn 라이브러리에 맞는 모델을 가져와 훈련하고 하이퍼파라미터를 찾아 완성된 모델을 테스트하여 모델의 성능을 측정한다. 그 과정에서 각 모델이 어떤 방식으로 구동되는지는 모른채 넘어갔다. 이 장에서는 그 모델 훈련이 어떻게 되는지를 배운다. 우리는 모델의 성능 지표를 먼저 보고, 그 값을 <u>최소화하는 $\theta$</u>를 찾는다.

***선형 회귀 모델의 MSE 비용 함수***   
$MSE\left(\mathbf{X},h_\theta\right) = \frac{1}{m}\sum_{i=1}^m \left(\theta^T \mathbf{x}^{(i)} - y^{(i)} \right)^2$

## 정규방정식

비용 함수를 최소화하는 $\theta$ 값을 찾는 해석적인 방법이다.

정규방정식<sup>normal equation</sup>

$\hat{\mathbf{\theta}} = \left( \mathbf{X}^T \mathbf{X} \right)^{-1} \mathbf{X}^T\mathbf{y}$

- $\hat{\mathbf{\theta}}$ : 비용 함수를 최소화하는 $\theta$
- $\mathbf{y}$ : 타깃 벡터

## 계산 복잡도

역행렬을 계산하는 계산복잡도 : $O\left(n^{2.4}\right) $ ~ $O\left(n^{3}\right) $ 사이

sklearn - LinearRegression Class의 SVD 방법의 계산복잡도 : $O\left(n^{2}\right) $

학습된 선형 회귀 모델은 예측이 매우 빠르다. 예측 계산 복잡도는 샘플 수와 특성 수에 선형적이다.
