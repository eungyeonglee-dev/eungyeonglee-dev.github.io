---
title: "chapter 5 optimization"
excerpt: ""

categories:
    - machine-learning
tags:
    - machine-learning
    - optimization
use_math: true
---


# 5장 최적화
5장 최적화에서 다루는 개념 : 컨벡스 셋,아핀 셋, 초평면, 반공간, 컨벡스 함수,헤시안 행렬, 준양정치행렬, 얀센의 부등식, 라그랑주 프리멀 함수, 라그랑주 듀얼 함수,
최소 제곱법, 뉴턴-랩슨 메소드, 선형 회귀, 로지스틱 회귀, 비용 함수, GD, SGD, mini-batch GD, momentum, Nesterov Accelerrated Gradient, Adagrad, Adadelta, RMSprop, Adam,AdaMax, Nadam

## 머신러닝의 궁극적인 목표는 최적화이다.
머신러닝에서 발생할 수 있는 문제는 다음과 같다. <br>
1. underfitting 2. slow learning 3. overfitting 이 문제를 해결하기 위한 방법으로 최적화가 쓰인다.

## 5.1 컨벡스 셋
공간 $\mathbb{R}^n$ 에서의 두 점 $x_1, x_2$를 잇는 선 사이에 있는 점 $x_1, x_2$ 을 아래와 같이 표현한다.
$y = wx_1 + (1-w)x_2$ <br>
직선 : 시작과 끝 지점이 존재하지 않는다. ($w \in \mathbb{R} $) <br>
선분 : 시작과 끝 지점이 존재한다. ($0 \le w \le 1$) <br>

가중치 $w$의 범위에 따라 직선인지 선분인지 결정된다. <br>

|속성|아핀셋(affine set)|컨벡스셋(convex set)|
|---|---|---|
|포함|직선($w \in \mathbb{R} $,범위제한없음)|선분($0 \le w \le 1$) |
|$n$차원 확장(조합)|아핀 조합(affine combination) <br> $w_1x_1+...+w_nx_n$ <br>조건: $w_1+...+w_n=1$|컨벡스 조합(convex combination) <br> $w_1x_1+...+w_nx_n$ <br>조건: $w_1+...+w_n=1$, $w_1,...,w_n>1$|
|   |   |컨벡스 헐: 주어진 점들을 포함하는 컨벡스 셋|

- 아핀 조합은 곧 집합 $C$에 속하는 점들의 선형 결합과 같다.
- 아핀 셋이면 컨벡스 셋이다.

### 초평면과 반공간
**초평면(hyperplane)** 은 *서포트 벡터 머신 알고리즘* 에서 핵심적인 개념이다.
머신러닝에서 *분류,회귀* 문제를 풀 때 핵심적인 개념이다.
**여러 머신러닝의 목적은 전체 공간을 효율적으로 나눌 수 있는 초평면을 찾는 문제**이다.

전체 공간은 초평면에 의해 두 개의 반공간(halfspace)로 나뉜다.<br>

|속성|초평면|반공간|
|---|---|---|
|집합 | $\{ \mathbf{x} \mid \mathbf{w}^T\mathbf{x} = \it{b} \}$ <br>$\mathbf{w}^T\mathbf{x} = \it{b}$ 를 만족하는 데이터 포인트 $\mathbf{x}$의 집합<br>$\mathbf{w}$는 $n$차원 가중치 벡터($\mathbf{w} \in \mathbb{R}^n$ ) <br> $b$는 1차원 스칼라값($b \in \mathbb{R}$) | ${ \mathbf{w}^T\mathbf{x} \le \it{b}}$ |

- 초평면에 의해 나뉜 반공간은 컨벡스 셋이다. 이유는 공간의 제한이 있기 때문이다.

## 5.2 컨벡스 함수

$f(\rm{w} \mathbf{x_1} + (1-w)\mathbf{x_2}) \le wf\mathbf(x_1) + (1-w)f(\mathbf{x_2})$



















## 5.3 라그랑주 프리멀 함수
## 5.4 라그랑주 듀얼 함수
## 5.5 KTT 조건
## 5.6 머신러닝에서 최적화 문제
## 5.7 뉴턴-랩슨 메소드
## 5.8 그래디언트 디센트 옵티마이저
